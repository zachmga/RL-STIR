model:
  d_model: 512
  log_layers: 8
  nhead: 8
  ff: 2048
  gnn_hidden: 256
  vocab_size: 32000
  max_seq_len: 1024
  graph_max_nodes: 5000

train:
  fp16: true
  batch_size: 8
  batch_tokens: 32000
  grad_checkpoint: true
  lr: 0.0003
  weight_decay: 0.01
  warmup_steps: 1000
  max_epochs: 10
  steps_per_epoch: 100
  log_interval: 10
  save_interval: 5
  max_grad_norm: 1.0

data:
  max_seq_len: 1024
  graph_max_nodes: 5000
  num_workers: 4
  pin_memory: true
  num_train_episodes: 800
  num_val_episodes: 200

optim:
  lr: 0.0003
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

loss:
  ttp_weight: 1.0
  tamper_weight: 0.5
  aux_weight: 0.1

# Output directory
output_dir: outputs/supervised

# Wandb configuration (optional)
wandb:
  enabled: false
  project: rl-stir
  name: null

# Tokenizer path (will be set after training)
tokenizer_path: null
