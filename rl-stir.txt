Awesome—let’s turn RL-STIR into a build plan you can actually ship. Below is a tight, staged roadmap that runs well on a single RTX 4060 today and scales cleanly to dual 3090s later.

0) Repo scaffold (1 hour)
rl-stir/
  README.md
  envs/                 # gym-style env + episode generator
  ingest/               # log collectors + parsers
  data/                 # parquet + graph edge lists (gitignored)
  modeling/             # encoders, heads
  rl/                   # PPO/A2C, rollout storage
  configs/              # yaml for 4060 vs 3090x2
  scripts/              # train_*.py, eval_*.py, profile_*.py

1) Environment & GPU sanity (4060 first)
# Ubuntu/WSL: CUDA 12.1 toolchain recommended
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install "torchvision>=0.18" torchtyping pyarrow pandas numpy scikit-learn pydantic \
            rich tqdm hydra-core omegaconf transformers sentencepiece \
            pytorch-lightning==2.*  gymnasium dm-tree \
            torch-geometric  # if this errors, follow PyG’s wheel instructions for cu121


Quick smoke test:

python - <<'PY'
import torch; print("CUDA:", torch.cuda.is_available(), "Device:", torch.cuda.get_device_name(0))
PY


Ultra-quick GPU watch (paste-as-one-line):

watch -n 0.3 'nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,utilization.memory,temperature.gpu,memory.used,memory.total,pstate,clocks.sm --format=csv,noheader'

2) Data contracts (keep it simple)

Parquet rows (data/logs/*.parquet)

host, ts_ns, channel (sysmon/auditd/auth/evtx), template_id, pid, ppid, tid,

exe, cmdline, user, sid, event_id, fields_json (raw kv), label_ttp (optional), tamper_tag (optional)

Graph edge lists (data/graphs/CASE_ID/)

proc_nodes.parquet: pid, first_ts, last_ts, exe_hash, signed, rare_parent, …

file_nodes.parquet: path, sha256, entropy, …

sock_nodes.parquet: l4, src_ip, dst_ip, dst_port, asn, geo, …

edges.parquet: src_id, dst_id, etype in {PROC_CHILD, PROC_FILE_RW, PROC_SOCK_CONN}

3) Ingest & minimal parsers (day 1)

Linux: parse audit.log, auth.log, journalctl exports → normalize into the schema.

Windows lab (later): Sysmon EVTX → parquet (use evtx dump → json → parquet).

Implement ingest/templater.py: map raw records → template_id + fixed field order for tokenization.

4) Tiny synthetic episode generator (no lab needed yet)

Build envs/sim/gen.py that stitches benign background with one of: phishing→script→LOLBin→C2, SSH brute, miner, ransomware.

Create tamper variants: drop events from one channel, jitter timestamps, orphan child procs, blank bash history.

Emit both logs and graph; also emit ground truth: gt_ttp_sequence.json, gt_tamper_spans.json.

5) Tokenizer & collator (GPU-friendly)

Text = [channel] [event_id] [template_id] [exe_basename] [rare_parent] … [truncated_cmdline_tokens].

Use a SentencePiece (32k) trained on your templates & exe names.

Collate to:
{"log_tokens": (B, T), "log_mask": (B, T), "graph": pyg_graph, "aux": stats} with pinned memory, non_blocking=True.

6) Encoder & heads (fits on 4060)

Log Transformer: 8 layers, d=512, nhead=8, ff=2048, SDPA/FlashAttention enabled automatically in PyTorch 2.x.

Graph: 2-layer GraphSAGE (hidden 256) over the proc/file/socket hetero graph.

Fuse: concat → LayerNorm → Linear(512+256→768) → GELU.

Heads:

policy/value for RL actions,

ttp_head (80 labels),

tamper_head (binary per token; optional span classifier).

7) Supervised pretrain (1–3 hours on 4060)
python scripts/train_supervised.py \
  configs/supervised_4060.yaml \
  data_dir=data/ \
  optim.lr=3e-4 train.fp16=true train.batch_tokens=32_000 \
  model.d_model=512 model.log_layers=8 gnn.hidden=256


Loss = CE(TTP) + BCE(tamper) + aux (calibration/ECE). This warms up the shared encoder.

8) Action catalog + RL env

configs/actions.json (start with ~24 actions):

QUERY:proc_tree(pid, Δt), QUERY:file_writes(exe, Δt), QUERY:rare_parent(exec), QUERY:socket_dst(asn), …

Costs per action (I/O heavy queries cost more).

envs/rl_soc_env.py: maintains a visibility mask; actions reveal new slices of logs/graph; reward = detection F1 & reduced uncertainty – cost.

9) PPO training loop (still OK on 4060)
python scripts/train_ppo.py \
  configs/ppo_4060.yaml \
  rollout.steps=128 rollout.minibatches=4 entropy_coef=0.01 gae_lambda=0.95 clip_range=0.2 \
  train.fp16=true train.grad_accum=2 train.max_grad_norm=1.0


Tips for 8 GB VRAM:

Use train.batch_tokens cap and gradient checkpointing for the transformer.

Keep graphs small: subgraph around frontier PIDs (≤5k nodes/episode).

Use bf16 if stable; otherwise fp16 autocast.

10) Evaluation & ablations
python scripts/eval.py configs/eval.yaml ckpt=... \
  metrics=[risk_aupr, ttp_f1, tamper_f1, ttd@K, cost@K]


Run ablations:

No-graph, no-transformer, no-RL (greedy queries), supervised-only.

11) Scale-up path to dual 3090s (24 GB x2)

Code changes: minimal. Switch config + launcher.

Launcher (single node, 2 GPUs):

CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 scripts/train_ppo.py configs/ppo_3090x2.yaml


3090 config knobs:

model.d_model=768, log_layers=12, nhead=12, ff=3072

train.batch_tokens 2–3× larger; remove grad checkpointing if not needed.

Increase graph radius (more nodes/edges).

DDP with find_unused_parameters=false; set torch.set_float32_matmul_precision("high").

Env for stability on PCIe-only rigs:

export NCCL_P2P_DISABLE=1 (if you see P2P issues)

export NCCL_IB_DISABLE=1 (no InfiniBand)

export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256,expandable_segments:True

Mixed precision: prefer bf16 on 3090 if drivers/toolkit support; otherwise fp16 with dynamic loss scaling.

Checkpoint compatibility:

Keep head dimensions identical; only widen depth/width. Load with strict=False to reuse the 4060 weights.

12) Real traces (after sim is green)

Spin a lab (Ubuntu server + Windows VM with Sysmon).

Replay Atomic Red Team / Caldera plans; export logs; label coarse TTPs per episode.

Generate “tampered” twins for each episode (your counter-forensics set).

Config stubs (drop into configs/)

supervised_4060.yaml

model:
  d_model: 512
  log_layers: 8
  nhead: 8
  ff: 2048
  gnn_hidden: 256
train:
  fp16: true
  batch_tokens: 32000
  grad_checkpoint: true
data:
  max_seq_len: 1024
  graph_max_nodes: 5000


ppo_3090x2.yaml

model:
  d_model: 768
  log_layers: 12
  nhead: 12
  ff: 3072
  gnn_hidden: 384
rl:
  algo: ppo
  steps: 256
  minibatches: 8
  clip: 0.2
  entropy_coef: 0.01
train:
  bf16: true
  batch_tokens: 120000
  grad_accum: 1
data:
  max_seq_len: 1536
  graph_max_nodes: 20000
ddp:
  backend: nccl

What you can implement today (order of attack)

1. Scaffold + env install (Section 0–1).

2. Synthetic generator + schema (Sections 3–4).

3. Tokenizer/collator (Section 5).

4. Small encoder + heads (Section 6).

5. Supervised warmup (Section 7).

6. Action catalog + RL env (Section 8).

7. PPO loop on 4060 (Section 9).

8. Eval + ablations (Section 10).

9. Flip to 3090x2 config & torchrun (Section 11).

10. Swap in real traces (Section 12).

